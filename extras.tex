\begin{appendices}
    \section{Bootstrap sampling}
        Пусть $X_1, \dots, X_n$ -- выборка. Генерим семплы размера $n$ из этой выборки (равномерно по имеющимся значениям). Делаем $B\sim 10^3$ т.н. bootstrap samples.
        По каждой подвыборке оцениваем регрессию и получаем $B$ разных оценок $\beta_i$ в $y = \beta_0 + \beta_1 X$.

        Для временных рядов: как попало семплить не можем, т.к. данные упорядоченны во времени.
        Делим ряд на набор блоков и применяем к ним bootstrap sampling. Если блоки маленькие, то зависимости во времени примерно сохраняются. 
        Такой подход называется \emph{block bootstrap sampling}.

    \section{Информационные критерии}
        Информационные критерии: чем меньше число, тем лучше модель.
        \begin{definition}[Информационный критерий Акаике]
            Пусть $k$ -- число оцениваемых параметров модели, $\mathcal{L}^* = \mathcal{L}(\hat\theta)$ -- правдоподобие полученной оценки. Тогда 
            \begin{equation*}
                \mathrm{AIC} = 2k - 2\ln \mathcal{L}^*.
            \end{equation*}
        \end{definition}

        \begin{definition}[Информационный критерий Шварца-Байеса]
            Пусть $k$ -- число оцениваемых параметров модели, $n$ -- число наблюдений, $\mathcal{L}^* = \mathcal{L}(\hat\theta)$ -- правдоподобие полученной оценки. Тогда 
            \begin{equation*}
                \mathrm{BIC} = k\ln n - 2\ln \mathcal{L}^*.
            \end{equation*}
        \end{definition}

        
\end{appendices}

